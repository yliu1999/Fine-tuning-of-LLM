# train_fixed.py
import os
import json
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨è„šæœ¬å¼€å§‹æ—¶è®¾ç½®ï¼‰
os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"] = "1"


def load_training_data(use_subset=True, subset_size=500):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    data_file = 'data/train.jsonl'

    with open(data_file, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]

    if use_subset:
        data = data[:subset_size]
        print(f"âš ï¸ ä½¿ç”¨æ•°æ®å­é›†è¿›è¡Œè®­ç»ƒ: {len(data)} æ¡")
    else:
        print(f"âœ… ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ: {len(data)} æ¡")

    training_data = []
    for item in data:
        if item['input'].strip():
            text = f"### Instruction:\n{item['instruction']}\n\n### Input:\n{item['input']}\n\n### Response:\n{item['output']}"
        else:
            text = f"### Instruction:\n{item['instruction']}\n\n### Response:\n{item['output']}"
        training_data.append({"text": text})

    return Dataset.from_list(training_data)


def main():
    print("=== åŒ»ç–—å¤§æ¨¡å‹å¾®è°ƒè®­ç»ƒå¼€å§‹ ===")

    # åŠ è½½è®­ç»ƒæ•°æ®
    dataset = load_training_data(use_subset=True, subset_size=500)

    # --- 1. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
    model_name = "Qwen/Qwen2-1.5B-Instruct"

    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # --- 2. é…ç½®LoRA ---
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
    )

    model = get_peft_model(model, peft_config)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(
        f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,} | æ€»å‚æ•°: {total_params:,} | å æ¯”: {100 * trainable_params / total_params:.4f}%")

    # --- 3. å®šä¹‰è®­ç»ƒå‚æ•° ---
    training_args = TrainingArguments(
        output_dir="./outputs",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        learning_rate=1e-4,
        num_train_epochs=3,
        logging_dir="./logs",
        logging_steps=10,
        save_strategy="epoch",
        fp16=True,
        optim="adamw_torch",
        max_grad_norm=1.0,
        warmup_steps=100,
        report_to=None,
        dataloader_pin_memory=False,
        # ç¦ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå•GPUï¼‰
        no_cuda=False,  # ç¡®ä¿ä½¿ç”¨CUDA
        local_rank=int(os.getenv("LOCAL_RANK", -1)),  # ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ
    )

    # --- 4. åˆå§‹åŒ–Trainer ---
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=512,
        tokenizer=tokenizer,
        packing=False,
    )

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return

    # --- 6. ä¿å­˜é€‚é…å™¨æƒé‡ ---
    output_dir = "./outputs/medical_lora_adapter"
    trainer.model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"ğŸ’¾ é€‚é…å™¨æƒé‡å·²ä¿å­˜åˆ°: {output_dir}")


if __name__ == "__main__":
    main()